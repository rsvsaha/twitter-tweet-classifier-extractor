{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "TwitterModel.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_uTWapw7I_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9uVLSU49gdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "saved_df= pd.read_csv('ANI_Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFQylRJ96_hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = saved_df['tweet_text']\n",
        "labels = []\n",
        "for label in saved_df['tweet_labels']:\n",
        "  if(label):\n",
        "    labels.append(1)\n",
        "  else:\n",
        "    labels.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Bg5OMA8idS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter below\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192): \n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "                    # f.flush()\n",
        "    return local_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSx0j-_59qbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removePunctuations(sentence):\n",
        "  punctList = '''!()-[]{};:'\"\\,./?@#$%^&@*_~'''\n",
        "  withoutPunctuation=\"\"\n",
        "  for character in sentence:\n",
        "    if(character not in punctList):\n",
        "      withoutPunctuation+=character\n",
        "  return withoutPunctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erte8l__Aok4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCommonWords():\n",
        "  bagOfWords = {}\n",
        "  download_file('https://www.gutenberg.org/files/766/766-0.txt')\n",
        "  f= open('766-0.txt','r')\n",
        "  for line in f.readlines():\n",
        "    s = line.lower()\n",
        "    s = s.strip()\n",
        "    s = removePunctuations(s)\n",
        "    s = s.split(' ')\n",
        "    for word in s:\n",
        "      if(word in bagOfWords.keys()):\n",
        "        bagOfWords[word]+=1\n",
        "      else:\n",
        "        bagOfWords[word]=0\n",
        "  bagOfWords = {key:value for key,value in sorted(bagOfWords.items(),key=lambda item:item[1],reverse=True)}\n",
        "  commonWords = [k for (k,v) in bagOfWords.items() if v>900 ]\n",
        "  return commonWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlMO26b7kJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "commonWords = getCommonWords()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdUjFMnD8hao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanSentence(sentence):\n",
        "  s = sentence.lower()\n",
        "  s = [re.sub(r'http\\S+','',s)]\n",
        "  s = [re.sub(r'[^A-Za-z0-9 ]','', s[0])]\n",
        "  s = [re.sub(' +', ' ', s[0])]\n",
        "  s = removePunctuations(s[0])\n",
        "  s = [word for word in s.split(' ') if(word not in commonWords)]\n",
        "  s = ' '.join(s)\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OwQbOuTFRFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fb6c9c18-1324-4ad5-9ff9-a117c992938d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def tokenize_and_stem_data(newData):\n",
        "  tokenized_data = []\n",
        "\n",
        "  for sentence in newData:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokenized_data.append(tokens)\n",
        "\n",
        "  data = tokenized_data\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  stemmedData = []\n",
        "  for sentence in data:\n",
        "    wordArray = []\n",
        "    for word in sentence:\n",
        "      # print(word)\n",
        "      word = stemmer.stem(word)\n",
        "      wordArray.append(word)\n",
        "    stemmedData.append(' '.join(wordArray))\n",
        "\n",
        "  return stemmedData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0RGDLZqGm0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "vectorizer = TfidfVectorizer()\n",
        "pca = PCA(n_components = 100)\n",
        "def vectorize_and_pca(data):\n",
        "  X = vectorizer.fit_transform(data)  \n",
        "  X = pca.fit_transform(X.toarray())\n",
        "  return X\n",
        "def transform_to_vectorize_pca(data):\n",
        "  X = vectorizer.transform(data)  \n",
        "  X = pca.transform(X.toarray())\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21hha7iACBWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleanedData = []\n",
        "for sentence in data:\n",
        "  sentencecleaned = cleanSentence(sentence)\n",
        "  cleanedData.append(sentencecleaned)\n",
        "\n",
        "#After Cleaning Tokenize and Stem it\n",
        "\n",
        "tokenized_and_stemmed_data = tokenize_and_stem_data(cleanedData)\n",
        "\n",
        "X = vectorize_and_pca(tokenized_and_stemmed_data) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXYDBQh0n69m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,labels,test_size=0.25,random_state=42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E3BfIeIo7sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "acc640e9-c819-49c2-d790-1d283cc526df"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier =  LogisticRegression()\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUKEKAM4pb0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted=classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXc8ZsiFpeQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "52790001-e313-43ee-be05-5ffbb193f95d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test,predicted)\n",
        "print(cm)\n",
        "accuracy = (cm[0][0]+cm[1][1])/len(y_test)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[321  26]\n",
            " [ 94  81]]\n",
            "0.7701149425287356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1N3xZTTCgrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb8a7837-b8d2-4d5b-c006-03839c0fc69e"
      },
      "source": [
        "from joblib import dump\n",
        "dump(classifier,'classifier.joblib')\n",
        "dump(pca,'pca.joblib')\n",
        "dump(vectorizer,'vectorizer.joblib')\n",
        "dump(commonWords,'commonwords.pkl') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['commonwords.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF-0zmUIElL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('classifier.joblib')\n",
        "files.download('pca.joblib')\n",
        "files.download('vectorizer.joblib')\n",
        "files.download(\"commonwords.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcXvmxVCphSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unknownsample = 'A 16-year-old girl from Salmara Bongaigaon, a secondary contact of a Markaz attendee, has tested #COVID19 positive. Number of #COVID19 patients now stands at 36: Assam Health Minister Himanta Biswa Sarma'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRZRetAcrkQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92710e8f-c89d-43ed-eeee-10c84b27a39c"
      },
      "source": [
        "cleanedSample =  cleanSentence(unknownsample)\n",
        "cleanedSample = [cleanedSample]\n",
        "cleanedSample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['16yearold girl salmara bongaigaon secondary contact markaz attendee has tested covid19 positive number covid19 patients now stands 36 assam health minister himanta biswa sarma']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGpDe0EYrzBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_and_stem_sample = tokenize_and_stem_data(cleanedSample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5E8WNus53i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfd86678-60bc-4ad8-cd58-6463c175c2ce"
      },
      "source": [
        "vectorized_pca_sample = transform_to_vectorize_pca(tokenize_and_stem_sample)\n",
        "classifier.predict(vectorized_pca_sample)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOIaV9A1s7Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('test_file.csv')\n",
        "new_data = df['tweet_text']\n",
        "new_labels = []\n",
        "for label in df['tweet_labels']:\n",
        "  if(label):\n",
        "    new_labels.append(1)\n",
        "  else:\n",
        "    new_labels.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLw7AFZ8to9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "ctr=1\n",
        "for data in new_data:\n",
        "  cleanedSample =  cleanSentence(data)\n",
        "  cleanedSample = [cleanedSample]\n",
        "  tokenize_and_stem_sample = tokenize_and_stem_data(cleanedSample)\n",
        "  vectorized_pca_sample = transform_to_vectorize_pca(tokenize_and_stem_sample)\n",
        "  predictedclass = classifier.predict(vectorized_pca_sample)[0]\n",
        "  predictions.append(predictedclass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XszE3J_Vym0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e34de2fd-5a5e-45f2-89d7-227c33077409"
      },
      "source": [
        "cm=confusion_matrix(new_labels,predictions)\n",
        "print(cm)\n",
        "accuracy = (cm[0][0]+cm[1][1])/len(predictions)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[225  15]\n",
            " [ 73  66]]\n",
            "0.7678100263852242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVFVRibcysdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataValue=[]\n",
        "for i in range(len(predictions)):\n",
        "  dataValue.append([new_data[i],new_labels[i],predictions[i]])\n",
        "newDf = pd.DataFrame(dataValue,columns=['tweet','actual_label','predicted_label'])\n",
        "newDf.to_csv('Predicted_Datasets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lxzn8sTFG_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('Predicted_Datasets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBC4IctZFafv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}